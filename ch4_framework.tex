\section{The UCT Algorithm}

UCT(Upper Confidence bound applied to Trees) (Kocsis, Szepesvari 2006) is a planning algorithm that takes
advantage of Monte-Carlo sampling to narrow down the search space during each iteration as an action is chosen.
This algorithm provides many benefits in that it can handle large search-spaces, stop at any point and return back
the best answer based on the Monte-Carlo simulations done till that point, and can have sub-optimal outputs.  The
last benefit may actually seem like a weakness, however, in natural language generation, it is beneficial to not have
a guarantee of generating the best sentence each time.  If it were the case that our planner was optimal, the output
would always be the same given matching inputs which is not a realistic output.

The goal of the UCT algorithm is to select the optimal or near-optimal sequence of actions from the start to end state.
This is done in an iterative manner with a single action chosen at each iteration.  In this the algorithm, the simulations
performed in the main loop are used to estimate the value of each action.  These simulations can be broken down into
4 steps.

\subsection{Tree Policy}
Starting from the current state, an action will repeatedly be chosen based on a balance of exploration and exploitation,
until a state with an open action is hit, a leaf is hit, of the depth limit is reached.  The balance between exploration and
exploitation is maintained by assigning each of the actions a probability based on its expected value as well as how often
it has been executed for the number of times the current state has been reached.  The exact formula used is the following:

$P(a) = Q(s,a) + c\sqrt{\frac{ln n(s)}{n(s,a)}}$

where s is the current state, a is the action from the current state, c is the exploration constant that must be tuned
empirically in practice, n(s) is the number of times state s has been encountered, and n(s,a) is the number of times action
a was taken in state s.

\subsection{State Expansion}
If there are any open actions for the current state, choose one of the open actions at random.  This will remove the
chosen action from the open action list for that state.

\subsection{Default Policy}
Continue to select actions at random until either a terminal state or the depth limit is reached.

\subsection{Reward Assignment}
Calculate the reward function for the state that results after executing all of the chosen action and push the reward all
the way back up to the root node representing the current state.

Once sufficient simulations have been run, the action with the largest value expectation is chosen.  The expectation of the
value of an action $a_i$ is computed by taking the average of all of the values computed for each simulation that began with action $a_i$.

Figure \ref{uct} provides an illustration of the UCT algorithm.  It shows a view of the state search tree at the end of an
intermediate Monte-Carlo simulation.  The root of this tree represents that current state and its child nodes are the possible
actions that UCT will be choosing from.  As the figure shows, the tree policy contains all of the node that have been expanded
at least once.  During the tree policy, we will traverse down the tree until we hit a leaf or a state containing an open actions.
In this example, the traversal follows the blue line and stops at the white node on the left since it has an open action.  Next, for
the State Expansion step, an open action is chosen at random and is denoted with the star shape in the figure.  Then, for the
Default Policy, random actions are taken until the depth limit is reached or there are no more open actions.  Each of the random
actions taken are denoted with a diamond shape and the terminal node is denoted as a square in the figure.  Once this happens,
the reward for the square node is calculated(in this case 1) and propagated up to the nodes in the tree policy along the blue line.
This process is repeated for the number of iterations specified when starting UCT and at each iteration, there is a possibility of adding
one more state to the tree policy.

The modification made to UCT in this work deals with the available actions to choose from.  In the default policy, instead of choosing
an action at random from all possible actions, the next action is only chosen from the new actions that resulted from executing the
previous action.  The motivation for this change is that by limiting the number of actions allowed, the final value computed at the end
of the action sequence will be a better representation of the value expectation for the first action in the sequence.  This provides the
added benefit that the algorithm is now selecting from a pruned search space allowing for faster execution.