In this chapter, we provide some background on grammars which we considered for use in this work.

\section{Markov Decision Processes}
A Markov Decision Process (MDP)~\cite{puterman_1994_markov}
is a tuple $(S, A, T, R, \gamma)$ where $S$ is a
set of states, $A$ is a set of actions available to an agent,
$T:S\times A\times S \rightarrow (0,1)$ is a possibly stochastic
function defining the probability $T(s,a,s')$ with which the
environment transitions to $s'$ when the agent does $a$ in state $s$.
$R:S\times A \rightarrow \mathbb{R}$ is a real-valued reward function that
specifies the utility of performing action $a$ in state $s$. Finally,
$\gamma$ is a discount factor that allows planning over infinite
horizons to converge. In such an MDP, the agent selects actions at
each state (a {\em policy}) to optimize the expected long-term
discounted reward: $\pi^*(s)=\arg \max_a E(\sum_t \gamma^t
R(s_t,a_t)|s=s_0)$, where the expectation is taken with respect to the
state transition distribution.

When the MDP model ($T$ and $R$) is
known, various dynamic programming algorithms such as value
iteration~\cite{bellman_1957_dynamic} can be used to plan and act in an MDP. When the
model is unknown, and the task is to formulate a policy, it can be
solved in a model-free way (i.e. without estimating $T$ and $R$)
through temporal difference (TD) learning. The key idea in TD-learning
is to take advantage of Monte Carlo sampling; since the agent visits
states and transitions with a frequency governed by the unknown
underlying $T$, simply keeping track of average rewards over time
yields the expected values required to compute the optimal actions at
each state.

Determining the optimal policy at {\em every} state using the above
strategy is polynomial in the size of the state-action
space~\cite{brafman_2003_rmax}, which is intractable in our case. But for our application, we do not
need to find the optimal policy. Rather we just need to {\em plan} in
an MDP to achieve a {\em given} communicative goal. Is it
possible to do this without exploring the entire state-action space?
Recent work answers this question affirmatively. New techniques such
as sparse sampling~\cite{kearns_1999_sparse} and
UCT~\cite{kocsis_bandit_2006} show how to generate near-optimal plans
in large MDPs with a time complexity that is independent of the state
space size. 

\section{Natural Language Grammars}

Natural languages like English are well-known to have grammars which are difficult
to represent with any single given formalism.  Many attempts have been made to
create a grammar which can sufficiently define natural language, the XTAG project
chiefly among them, but even those projects fail to sufficiently embody the
constraints that spoken and written English put on word orderings and meaning.

\subsection{Context-Free Grammars}

One of the simplest grammars which can convey relationships between terminals
and nonterminals is the "Context Free Grammar" (CFG).  A CFG is made up of rules,
each of which specifies a possible rewrite of a nonterminal into one or more
terminals or nonterminals.  Once there are no further nonterminals to be rewritten,
the final state is a series of terminals which is in the language defined by the grammar.
This language is defined as the set of all possible generated sentences.\\

CFGs are so named because the rules in them are unable to consider the "context" for
their rewriting.  No rules may condition on the presence or placement of terminals
or nonterminals other than the single nonterminal to be rewritten.  This is, of course,
insufficiently expressive to be a grammar for English (or any other natural language).\\

\subsection{Probabilistic CFGs}

One attempt to make CFGs more realistic for parsing or generating natural languages was to
introduce a probabilistic component.  In a Probabilistic Context Free Grammar (PCFG), CFG
rules are tagged with probabilities, usually representing the frequency of their appearance in
an observed corpus.  These probabilities are required to sum to 1 for each nonterminal to
be rewritten, so that there is a defined probability distribution over the options for each nonterminal.\\

This grammar type does appropriately convey the crucial truth that that not all rules are equal.
However, it does so in a naive way, unable to express these probabilities in terms of the placement
of the nonterminal in a sentence, and consequently also unable to successfully express English.

\subsection{Tree Adjoining Grammars}

A Tree Adjoining Grammar (TAG) takes a different approach, differing substantially from CFGs and PCFGs.
TAGs are tree-based grammars consisting of two sets of trees, called initial
trees and adjoining trees (sometimes ``auxiliary trees").  These two kinds of trees tend to perform
different roles semantically in addition to their differing syntactic roles.  The former,
initial trees, are usually for adding new semantic information to the sentence.  They
add new nodes to the sentence tree.  In a simplified TAG of English,
initial trees contain rules like "Verb Phrases contain a Verb and a Noun", or "VP $->$ V N".
A sentence can be made entirely of initial trees, but a sentence must contain at least
one initial tree.  An example of an initial tree is shown in Figure $<$SOMETHING$>$.\\
(S (NP (D) (N (Cat))) (VP))\\
This tree has as its root the S node, and this defines how it can interact with other
trees under a TAG.  Since this is an initial tree, it can only interact with other trees by
substitution.  That is, this tree is a drop-in replacement for an S node with no children.
This is how we get from our stub sentence (S) to a complete sentence.

Adjoining trees usually clarify a point in a sentence.  In a simplified TAG of English, adjoining
trees would contain rules like "a noun can have an adjective placed in front of it," or "N $->$ A N".
An example of an adjoining tree is shown in Figure $<$SOMETHING$>$.\\
(N (A (Red)) (N*))\\
This tree has as its root an N node.  It also has a specially annotated N node elsewhere in the
tree.  These nodes define its interaction with other trees under a TAG.  Adjoining trees interact
with other trees only by "adjoining".  In an adjoining action, you select the node
to adjoin to, which must be of the same label as the root node of the adjoining tree.  You remove
that node from the other tree and put the adjoining tree in its place.  Then
you place that original node into the adjoining tree as a substitution for the foot node.
For example, if we had the trees\\
(S (NP (D (the)) (N (cat))) (VP))\\
and we wanted to adjoin the example adjoining tree above, we would first create this intermediate tree:\\
(S (NP (D (the)) (N (A (red) (N*)))) (VP))\\
And then perform the substitution:\\
(S (NP (D (the)) (N (A (red) (N (cat))))) (VP))\\
Notice that this has the effect, in all cases, of making the tree deeper.

We use a variation of TAGs in our work, called a lexicalized TAG (LTAG), where each tree is
associated with a lexical item called an anchor.  All examples given above are examples of
lexicalized trees.  An example of an unlexicalized tree would be (NP (D) (N)), where there
are no nodes containing lexical tokens.\\

As with CFGs, an attempt to make TAGs more tractable for generation or parsing was to
introduce probabilities.  Each tree rule in a TAG must be annotated with a probability, and
there will be a probability distribution for each nonterminal at the frontier of the tree.
These probabilistic TAGs, or PTAGs, can be useful if a corpus of text is available.\\

The XTAG project has had some success using LTAGs to model English grammar, 
so we focus mostly on LTAGs in our work.