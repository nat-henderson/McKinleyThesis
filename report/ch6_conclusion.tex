In conclusion, we will compare STRUCT to established NLG methods,
discuss other strengths and weaknesses of STRUCT, and describe
directions for future work.

We believe that we have produced an algorithm which unifies the
two approaches to NLG discussed in Chapter 3.  
We have fused the probabilistic reasoning and
domain knowledge use from the probabilistic ranking method with
the planning problem conversion and structured approach to semantics
from the classical planning approach.  We have the advantage of
partial goal satisfaction from ``overgenerate and rank" and the
advantage of explicit semantic meaning specification and
output from classical planning.

We have avoided the all-or-nothing weakness of classical planning,
where a classical planner cannot emit a sentence which does not
optimally satisfy the goal.  Yet, STRUCT allows generation of complex
goals, which would be intractable with a probabilistic ranking generator.
We have avoided the requirement that we specify a large percentage of
our output as input, which is a problem with probabilistic ranking.

In short, we have constructed an algorithm with many of the strengths
of both classical planning and probabilistic ranking, and few of the
weaknesses of either.

STRUCT's prime strength is its ability to partially satisfy goals in cases where
perfectly correct generation is impossible, either because of conflicting
goals or because of a grammar which does not allow for all goals.
This allows STRUCT to be used in situations even without perfect knowledge
of the domain, since STRUCT can recover from some of the issues that come
from unknown domains (like a grammar which is too small or insufficient knowledge
of the world), and continue attempting to generate close-to-optimal output
where other generators would either fail after churning on the problem
for a long time, or emit something nonsensical.

Another important strength of STRUCT is its anytime nature; something which
we have not seen done before.  STRUCT is capable of creating an approximate
solution very quickly (usually less than 0.5 seconds), and that approximate solution
can be emitted if the user desires a response very quickly.  The solution will be iteratively
improved until it reaches a sentence which fulfills all communicative goals given.

One important weakness of STRUCT is the rapid increase in generation time
past certain limits.  STRUCT has trouble generating referring expressions of
length 10 or more, and has trouble generating sentences which adjoin
more than about 5 verbs.

STRUCT also is limited by its grammar.  It is difficult to produce an LTAG grammar
for English, and any such grammar will, by nature, overgenerate (the language
specified by the grammar will contain some constructs not acceptable in
English).  It is best to build a grammar which undergenerates (the language
specified by the grammar is a subset of acceptable English), but that can
be difficult without knowledge of which constructs exactly will be necessary.

Due to some peculiarities of the python interpreter, we were unable to
efficiently parallelize UCT during the search phase of generation.  This
would be relatively straightforward using a worker pool solution, and we
could drastically shrink the amount of time spent in that search.

We may also be able to use STRUCT as the output generator of a dialog system,
similar to NJFun \cite{litman_njfun_2000}, instead of the template-based generation
that most such systems employ.  STRUCT would be substantially more flexible in
its output than a template-based system, which can usually only respond to preprogrammed
error cases.  STRUCT's ability to partially accomplish communicative goals would be
valuable in this application.

We could consider using an alternative approach to adapting UCT to a DAG.  Instead
of treating states reached from differing parents as different states, we could keep
track of how they were reached each time they were reached and update their parent
states accordingly.  This would allow us to reduce the number of simulations needed
for high-depth experiments, especially experiments where adjoin operations are
performed repeatedly.

We could also consider using a different semantic model.  Our choice of first-order logic
formulas was encouraged by Koller's work, and allowed us to easily compare to CRISP.
However, other work has suggested a lambda-calculus semantic model \cite{wong2007learning}
which we feel holds promise.  Initial experimentation has suggested that drastic speed increases
may result from this choice of semantic representation.

We are also interested in reward functions.  The reward functions we have created
are good for generating language based on facts about the world, and would
serve for goal-directed communication, but those are not the only possible
goals of natural language generation.  If we could learn a reward function from
a corpus and attempt to emulate that corpus in our speech, or build a hierarchical
reward function which could attempt to accomplish secondary or tertiary goals
in addition to a primary one.  We should explore the possibilities that the
generality of our architecture afford us.

In conclusion, we have presented an algorithm which performs Natural Language
Generation in a well-principled way, unifying two popular schools of thought regarding
NLG.  Experimental evaluation shows that it performs as well as the state-of-the-art in
the field, and its nature allows for a substantially greater set of use cases than either
of the popular schools it synthesizes.