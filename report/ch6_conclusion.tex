In conclusion, we will compare STRUCT to established NLG methods,
discuss other strengths and weaknesses of STRUCT, and describe
directions for future work.

\section{Comparison to Competing NLG Methods}

As discussed in Chapter 3, there are two other popular methods of
solving the NLG problem.  In the first, the NLG problem is treated as a
classical planning problem and solved using an off-the-shelf planner.
In the second, the NLG problem is solved using an overgeneration
strategy which reduces to a dynamic programming approach.  See
chapter 3 for a more detailed explanation of these methods.

We believe that we have produced an algorithm which unifies these
two approaches.  We have fused the probabilistic reasoning and
domain knowledge use from the probabilistic ranking method with
the planning problem conversion and structured approach to semantics
from the classical planning approach.  We have the advantage of
partial goal satisfaction from ``overgenerate and rank" and the
advantage of explicit semantic meaning specification and
output from classical planning.

We have avoided the all-or-nothing weakness of classical planning,
where a classical planner cannot emit a sentence which does not
optimally satisfy the goal.  Yet, STRUCT allows generation of complex
goals, which would be intractable with a probabilistic ranking generator.
We have avoided the requirement that we specify a large percentage of
our output as input, which is a problem with probabilistic ranking.

In short, we have constructed an algorithm with many of the strengths
of both classical planning and probabilistic ranking, and few of the
weaknesses of either.

\section{STRUCT: Strengths and Weaknesses}

STRUCT's prime strength is its ability to partially satisfy goals in cases where
perfectly correct generation is impossible, either because of conflicting
goals or because of a grammar which does not allow for all goals.
This allows STRUCT to be used in situations even without perfect knowledge
of the domain, since STRUCT can recover from some of the issues that come
from unknown domains (like a grammar which is too small or insufficient knowledge
of the world), and continue attempting to generate close-to-optimal output
where other generators would either fail after churning on the problem
for a long time, or emit something nonsensical.

Another important strength of STRUCT is its anytime nature; something which
we have not seen done before.  STRUCT is capable of creating an approximate
solution very quickly (usually less than 0.5 seconds), and that approximate solution
can be emitted if the user desires a response very quickly.  The solution will be iteratively
improved until it reaches a sentence which fulfills all communicative goals given.

One important weakness of STRUCT is the rapid increase in generation time
past certain limits.  STRUCT has trouble generating referring expressions of
length 10 or more, and has trouble generating sentences which adjoin
more than about 5 verbs.

STRUCT also is limited by its grammar.  It is difficult to produce an LTAG grammar
for English, and any such grammar will, by nature, overgenerate (the language
specified by the grammar will contain some constructs not acceptable in
English).  It is best to build a grammar which undergenerates (the language
specified by the grammar is a subset of acceptable English), but that can
be difficult without knowledge of which constructs exactly will be necessary.

\section{Future Work}

Due to some peculiarities of the python interpreter, we were unable to
efficiently parallelize UCT during the search phase of generation.  This
would be relatively straightforward using a worker pool solution, and future
work could easily shrink the amount of time spent in that search drastically.

The dialog system which we have implemented is quite rudimentary
(simpler even than NJFun \cite{litman_njfun_2000}), and could
be improved to use more intelligent parsing techniques than simple
keyword searching.\\


In conclusion, we have presented an algorithm which performs Natural Language
Generation in a well-principled way, unifying two popular schools of thought regarding
NLG.  Experimental evaluation shows that it performs as well as the state-of-the-art in
the field, and its nature allows for a substantially greater set of use cases than either
of the popular schools it synthesizes.